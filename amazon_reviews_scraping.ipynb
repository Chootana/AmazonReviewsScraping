{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon_reviews_scraping",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYodqFN5jXUM39RZk432MV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chootana/AmazonReviewsScraping/blob/master/amazon_reviews_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVlErskzJB3d"
      },
      "source": [
        "!apt-get update \r\n",
        "!apt install chromium-chromedriver \r\n",
        "\r\n",
        "!pip install selenium \r\n",
        "!pip install beautifulsoup4\r\n",
        "\r\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3wsNWwFJfwn"
      },
      "source": [
        "# import \r\n",
        "import sys \r\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\r\n",
        "\r\n",
        "from selenium import webdriver\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "import time \r\n",
        "import pandas as pd "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILcTqrR2KBcO"
      },
      "source": [
        "class AmazonScraping():\r\n",
        "    def __init__(self):\r\n",
        "        pass \r\n",
        "\r\n",
        "    def get_page_from_(self, url):\r\n",
        "        \"\"\"\r\n",
        "        Parameters\r\n",
        "        -------\r\n",
        "        url: str\r\n",
        "\r\n",
        "        Returns\r\n",
        "        -------\r\n",
        "        text: str \r\n",
        "            source code for this url\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        text = \"\"\r\n",
        "        chrome_options = webdriver.ChromeOptions()\r\n",
        "        chrome_options.add_argument('--headless')\r\n",
        "        chrome_options.add_argument('--no-sandbox')\r\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\r\n",
        "        wd = webdriver.Chrome('chromedriver',options=chrome_options)\r\n",
        "\r\n",
        "        wd.get(url)\r\n",
        "        wd.implicitly_wait(10) \r\n",
        "        text = wd.page_source\r\n",
        "            \r\n",
        "        # stop browsing\r\n",
        "        wd.quit()\r\n",
        "        \r\n",
        "        return text\r\n",
        "\r\n",
        "    def get_all_reviews(self, url):\r\n",
        "        \"\"\"\r\n",
        "        Parameters\r\n",
        "        --------\r\n",
        "        url: str\r\n",
        "\r\n",
        "        Returns\r\n",
        "        --------\r\n",
        "        reviews: dict\r\n",
        "            ratings, titles, texts\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        reviews = {\r\n",
        "            'ratings': [],\r\n",
        "            'titles': [],\r\n",
        "            'texts': [],\r\n",
        "        }\r\n",
        "\r\n",
        "        # change the page to product-reviews page\r\n",
        "        url = url.replace('dp', 'product-reviews')\r\n",
        "        print('[URL] {}'.format(url))\r\n",
        "\r\n",
        "\r\n",
        "        idx = 1\r\n",
        "        while True:\r\n",
        "            res = self.get_page_from_(url)\r\n",
        "            soup_amazon = BeautifulSoup(res, features='lxml')\r\n",
        "\r\n",
        "            print(f'# {idx} searching')\r\n",
        "            review_ratings = soup_amazon.select('.review-rating')\r\n",
        "            review_titles = soup_amazon.select('.review-title-content')\r\n",
        "            review_texts = soup_amazon.select('.review-text')\r\n",
        "\r\n",
        "            for num in range(len(review_texts)):\r\n",
        "                reviews['ratings'].append(review_ratings[num].text.split(\" \")[0])\r\n",
        "                reviews['titles'].append(review_titles[num].text)\r\n",
        "                reviews['texts'].append(review_texts[num].text)\r\n",
        "            \r\n",
        "            idx += 1\r\n",
        "            time.sleep(1)\r\n",
        "\r\n",
        "            next_page = soup_amazon.select('li.a-last a')\r\n",
        "            if next_page != []:\r\n",
        "                url_next = 'https://www.amazon.co.jp/{}'.format(next_page[0].attrs['href'])\r\n",
        "                url = url_next\r\n",
        "                print('[URL] {}'.format(url))\r\n",
        "            else:\r\n",
        "                break\r\n",
        "        \r\n",
        "        print('[Scraping] finish.')\r\n",
        "        return reviews\r\n",
        "\r\n",
        "    def save_reviews_as_csv(self, reviews, path_csv='./reviews.csv'):\r\n",
        "        \"\"\"\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        df_reviews = pd.DataFrame.from_dict(reviews, orient='index').T\r\n",
        "        df_reviews.to_csv(path_csv, encoding='utf_8_sig')\r\n",
        "        print('[Save] save to {}'.format(path_csv))\r\n",
        "\r\n",
        "    def run(self, url, path_csv='./reviews.csv'):\r\n",
        "        \"\"\"\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        reviews = self.get_all_reviews(url)\r\n",
        "\r\n",
        "        for key, val in reviews.items():\r\n",
        "            assert val != [], '[False] There are no results.'\r\n",
        "        \r\n",
        "        self.save_reviews_as_csv(reviews, path_csv)\r\n",
        "        print('[Finish]')\r\n",
        "\r\n",
        "        \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSAEBY0QLTrk"
      },
      "source": [
        "#@title ## Amazon Reviews Scraping\r\n",
        "\r\n",
        "#@markdown --- \r\n",
        "#@markdown ### Enter URL\r\n",
        "url = \"https://www.amazon.co.jp/%E3%83%8B%E3%83%99%E3%82%A2-%E3%83%AA%E3%83%83%E3%83%97%E3%82%B1%E3%82%A2-%E3%83%93%E3%82%BF%E3%83%9F%E3%83%B3E-3-9g/dp/B001PM2L72/ref=sr_1_7_mod_primary_new?dchild=1&keywords=%E3%83%8B%E3%83%99%E3%82%A2+%E3%83%AA%E3%83%83%E3%83%97&qid=1608550669&sbo=RZvfv%2F%2FHxDF%2BO5021pAnSA%3D%3D&sr=8-7\" #@param {type:\"string\"}\r\n",
        "\r\n",
        "\r\n",
        "#@markdown --- \r\n",
        "#@markdown ### Enter Save Path\r\n",
        "#@markdown ex.) ./reviews.csv\r\n",
        "path_csv = \"./test.csv\" #@param {type:\"string\"}\r\n",
        "\r\n",
        "#@markdown --- \r\n",
        "\r\n",
        "\r\n",
        "amazon_scraping = AmazonScraping()\r\n",
        "amazon_scraping.run(url, path_csv)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPw1a8jjLZId"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}